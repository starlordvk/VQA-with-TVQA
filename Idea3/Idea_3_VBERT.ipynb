{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies and imports"
      ],
      "metadata": {
        "id": "oP1PECAwrLIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "2MSCTnluuGss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets"
      ],
      "metadata": {
        "id": "RPBIyMZqrIpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-tsiNF08_bE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, VisualBertForQuestionAnswering, VisualBertModel\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from nltk.corpus import stopwords\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download data from TVQA"
      ],
      "metadata": {
        "id": "ZvfBrV4tqjAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DOWNLOAD_PATH = \".\""
      ],
      "metadata": {
        "id": "iii9DF3dsPcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and extract QA data\n",
        "!wget https://tvqa.cs.unc.edu/files/tvqa_qa_release.tar.gz -P ${DOWNLOAD_PATH}\n",
        "!tar xzf ${BASE_PATH}/tvqa_qa_release.tar.gz -C ${DOWNLOAD_PATH}"
      ],
      "metadata": {
        "id": "5GLi9jCZsbqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and extract subtitles\n",
        "\n",
        "# TVQA\n",
        "!wget https://tvqa.cs.unc.edu/files/tvqa_subtitles.tar.gz -P ${DOWNLOAD_PATH}\n",
        "!tar xzf ${DOWNLOAD_PATH}/tvqa_subtitles.tar.gz  -C ${DOWNLOAD_PATH}\n",
        "\n",
        "# TVQA+\n",
        "wget https://tvqa.cs.unc.edu/files/tvqa_plus_subtitles.tar.gz  -P ${DOWNLOAD_PATH}/tvqa_plus/\n",
        "tar -xf ${DOWNLOAD_PATH}/tvqa_plus/tvqa_plus_subtitles.tar.gz -C ${DOWNLOAD_PATH}/tvqa_plus/"
      ],
      "metadata": {
        "id": "hhqLMWpJsjY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuP5ePiy8wwN"
      },
      "outputs": [],
      "source": [
        "# Download and extract resnet features\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1m8bC4lefQsP2tRhMLAaiy0AVuBXZtegc' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1m8bC4lefQsP2tRhMLAaiy0AVuBXZtegc\" -O tvqa_imagenet_resnet101_pool5_hq.tar.gz && rm -rf /tmp/cookies.txt\n",
        "!tar xzf tvqa_imagenet_resnet101_pool5_hq.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and extract visual concept features\n",
        "!wget http://tvqa.cs.unc.edu/files/det_visual_concepts_hq.pickle.tar.gz\n",
        "!tar -xvf det_visual_concepts_hq.pickle.tar.gz"
      ],
      "metadata": {
        "id": "FS7zcXm7qs-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and extract glove embeddings \n",
        "!wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip -P ${DOWNLOAD_PATH}\n",
        "!unzip -qqq ${DOWNLOAD_PATH}/glove.6B -C ${DOWNLOAD_PATH}\n",
        "!unzip ../glove.6B.zip"
      ],
      "metadata": {
        "id": "YcuY9hTNtNwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYj4Ad_O58Xi"
      },
      "source": [
        "## Model and training\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_types = [\"what\", \"who\", \"whom\", \"how\", \"where\", \"why\"]\n",
        "english_stopwords = stopwords.words('english')\n",
        "english_stopwords = list(filter(lambda i: i not in question_types, english_stopwords))\n",
        "english_stopwords.append(\"?\")"
      ],
      "metadata": {
        "id": "jcIFU8npxfau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vcpt = pickle.load(open('../det_visual_concepts_hq.pickle', 'rb'))\n",
        "# Create embedding dict\n",
        "embeddingDict = {}\n",
        "with open('./glove.6B.50d.txt') as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    arr = line.split()\n",
        "    embeddingDict[arr[0]] = np.asarray(arr[1:], dtype='float32')"
      ],
      "metadata": {
        "id": "7sQEjEm00ju-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nuG8CBC58Gy"
      },
      "outputs": [],
      "source": [
        "def extract_objects_from_frame(clip_name, frame_number):\n",
        "  objects = vcpt[clip_name][frame_number].split(\",\")\n",
        "  objects = list(map(lambda x: x.strip(), objects))\n",
        "  return objects\n",
        "\n",
        "def split_phrases(phrases):\n",
        "  split_phrase = set()\n",
        "  for phrase in phrases:\n",
        "    split_phrase.update(phrase.split())\n",
        "  return split_phrase\n",
        "\n",
        "def get_list_embedding(word_list):\n",
        "  embedding = []\n",
        "  for obj in word_list:\n",
        "    if (obj in embeddingDict):\n",
        "      embedding.append(embeddingDict[obj])\n",
        "  return np.array(embedding)\n",
        "\n",
        "def get_question_tokens(question):\n",
        "  words = nltk.word_tokenize(question)\n",
        "  words = list(filter(lambda x: x not in english_stopwords and x in embeddingDict, map(lambda x: x.lower(), words))) \n",
        "  return words\n",
        "\n",
        "def get_best_frames_start(match_scores, window_size):\n",
        "  max_sum = -1\n",
        "  index  = -1\n",
        "  for i in range(len(match_scores) - window_size):\n",
        "    curr_sum = np.sum(match_scores[i:i+ ws])\n",
        "    if curr_sum > max_sum:\n",
        "      max_sum = curr_sum\n",
        "      index  = i\n",
        "  return index\n",
        "\n",
        "def get_frame_scores(clip_name, question):\n",
        "  NUM_OBJECTS = 10\n",
        "  frame_scores = np.zeros(total_frames)\n",
        "  question_embedding = get_list_embedding(get_question_tokens(question))\n",
        "  for frame_number in range(len(vcpt(clip_name])):\n",
        "    score = 0\n",
        "    objects_in_frame = split_phrases(extract_objects_from_frame(clip_name, frame_number)[:NUM_OBJECTS])\n",
        "    frame_embedding = get_list_embedding(objects_in_frame)\n",
        "    if objects_in_frame:\n",
        "      score = np.sum(frame_embedding.dot(question_embedding.T))\n",
        "    frame_scores[frame_number] = score\n",
        "  return frame_scores\n",
        "\n",
        "def get_unique_objects_from_frame(clip_name, frame_start_index, window_size):\n",
        "  total_frames = len(vcpt[clip_name])\n",
        "  unique_objects = set()\n",
        "  for frame_number in range(frame_start_index, frame_start_index + window_size):\n",
        "    unique_objects.update(split_phrases(extract_objects_from_frame(clip_name, i)))\n",
        "  return list(unique_objects)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-U-TdY7G32d"
      },
      "outputs": [],
      "source": [
        "class TVQAPlus(torch.utils.data.Dataset):\n",
        "    def __init__(self, isTraining=True):\n",
        "        if isTraining:\n",
        "          QAFilePath = DOWNLOAD_PATH + \"/tvqa_plus/tvqa_plus_train.json\"\n",
        "        else:\n",
        "          QAFilePath = DOWNLOAD_PATH + \"/tvqa_plus/tvqa_plus_val.json\"\n",
        "\n",
        "        self.qa = {}\n",
        "        with open(QAFilePath) as f:\n",
        "          self.qa = json.load(f)\n",
        "\n",
        "        self.subtitles = {}\n",
        "        with open(DOWNLOAD_PATH + \"/tvqa_plus/tvqa_plus_subtitles.json\") as f:\n",
        "          self.subtitles = json.load(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qa)\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "      q, a0, a1, a2, a3, a4 = self.qa[i]['q'], self.qa[i]['a0'],  self.qa[i]['a1'], self.qa[i]['a2'], self.qa[i]['a3'], self.qa[i]['a4']\n",
        "      answer_idx = int(self.qa[i]['answer_idx'])\n",
        "      vid_name = self.qa[i]['vid_name']\n",
        "      subt_text = self.subtitles[vid_name]['sub_text']\n",
        "      return q, subt_text, a0, a1, a2, a3, a4, video_name, answer_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vAyQMnp9GBh"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\"\n",
        "class VBERT_Wrapper(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(VBERT_Wrapper, self).__init__()\n",
        "        self.model = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\",  output_hidden_states = True)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",  output_hidden_states = True)\n",
        "        self.proj = torch.nn.Sequential(torch.nn.Linear(768, 256), torch.nn.GELU(), torch.nn.Linear(256, 64), torch.nn.GELU(), torch.nn.Linear(64, 1)).to(device)\n",
        "        unfreezed_layers = ['encoder.layer.11','pooler.dense.weight', 'pooler.dense.bias']\n",
        "        for name, parameter in self.model.named_parameters():\n",
        "            freeze_layer = False\n",
        "            for layer in unfreezed_layers:\n",
        "                if layer in name:\n",
        "                    freeze_layer = True\n",
        "                    break\n",
        "            parameter.requires_grad = freeze_layer\n",
        "\n",
        "    def get_vid_feats_and_objects(clip_names, questions, window_size=10):\n",
        "      resnet_feats = []\n",
        "      objects_from_video = []\n",
        "      for clip_number, clip_name in enumerate(clip_names):\n",
        "          frame_scores = get_frame_scores(clip_name, questions[clip_number])\n",
        "          best_frames_start_index = get_best_frames_start(frame_scores, window_size)\n",
        "          unique_objects = get_unique_objects_from_frame(clip_name, best_frames_start_index, window_size)\n",
        "          objects_from_video.append(' '.join(unique_objects))\n",
        "          resnet_feats.append(torch.tensor(vid_h5[clip_name][best_frames_start_index:best_frames_start_index+window_size, :], device=\"cuda\"))\n",
        "      resnet_feats =  pad_sequence(resnet_feats, batch_first=True)\n",
        "      return resnet_feats, objects_from_video\n",
        "\n",
        "    def bert_forward(self, questions, options, subtitles, clip_names):\n",
        "        qa_representation = [questions[i]+ ' [SEP] ' + options[i] for i in range(len(questions))]\n",
        "        resnet_feats, objects_from_video = self.get_vid_feats_and_objects(clip_names, questions)\n",
        "        subitlte_representation = [subtitles[i]+ ' [SEP] ' + objects_from_video[i] for i in range(len(subtitles))]\n",
        "        token_ids = torch.ones(resnet_feats.shape[:-1], dtype=torch.long).to(device)\n",
        "        attention_mask = torch.ones(resnet_feats.shape[:-1], dtype=torch.float).to(device)\n",
        "        inputs = self.tokenizer(subitlte_representation, qa_representation, padding=\"max_length\", truncation=True, return_token_type_ids=True, return_attention_mask=True, add_special_tokens=True, return_tensors=\"pt\")\n",
        "        inputs.update({\"visual_embeds\": resnet_feats, \"visual_token_type_ids\": token_ids, \"visual_attention_mask\": attention_mask,})\n",
        "        inputs = inputs.to(device)\n",
        "        output = self.model(**inputs)\n",
        "        hidden_states = output.last_hidden_state\n",
        "        cls_tokens = hidden_states[:,0,:]\n",
        "        return cls_tokens\n",
        "\n",
        "    def forward(self, question, subt_text, options, video_names):\n",
        "        scores  = []\n",
        "        for i range(len(options)):\n",
        "          scores.append(self.proj(self.bert_forward(question=question, ans=options[i], subt_text=subt_text, video_names=video_names)))\n",
        "        return torch.tensor(scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "049VKQuMN5hZ"
      },
      "outputs": [],
      "source": [
        "train_dataset = TVQAPlus(isTraining=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataset = TVQAPlus(isTraining=False)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=dev_batch_size, shuffle=False)\n",
        "vbert_mod = VBERT_Wrapper()\n",
        "batch_size=24\n",
        "dev_batch_size=24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEh2Hhvlmefj"
      },
      "outputs": [],
      "source": [
        "def val_acc(model):\n",
        "  model.eval()\n",
        "  num_correct = 0\n",
        "  for batch_idx, (questions, subt_text, a0, a1, a2, a3, a4, video_names, answer_idx) in enumerate(val_loader):\n",
        "    answer_idx = answer_idx.to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "      logits = model.forward(questions, subt_text, [a0, a1, a2, a3, a4], video_names)\n",
        "    num_correct += int((torch.argmax(logits, axis=1) == answer_idx).sum())\n",
        "    acc = 100 * num_correct / ((batch_idx + 1) * dev_batch_size)\n",
        "  dev_acc = 100 * num_correct / (len(val_loader) * dev_batch_size)\n",
        "  model.train()\n",
        "  return dev_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIvDHgmTkg8l"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(vbert_mod.parameters(), lr=1e-3)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkZOjiffOp1m"
      },
      "outputs": [],
      "source": [
        "def get_fileName(epoch, isDev=False):\n",
        "  if isDev:\n",
        "    return \"best_acc_model.pth\"\n",
        "  else:\n",
        "    return \"train_\" + str(epoch) + \".pth\"\n",
        "  \n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "epoch = 1\n",
        "best_dev_acc = 0\n",
        "\n",
        "while epoch <= 10:\n",
        "  \n",
        "    num_correct = 0\n",
        "    loss_epoch = 0\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "    vbert_mod.train()\n",
        "\n",
        "    # Appending an \"s\" in front of all var names to make it clear that we are dealing with a batch\n",
        "    for batch_idx, (questions, subt_texts, a0s, a1s, a2s, a3s, a4s, video_names, answer_idx) in enumerate(train_loader):\n",
        "        logits = vbert_mod.forward(questions, subt_texts, [a0s, a1s, a2s, a3s, a4s], video_names)\n",
        "        answer_idx = answer_idx.to(device)\n",
        "        correct_prediction_count += int((torch.argmax(logits, axis=1) == answer_idx).sum())\n",
        "        loss = criterion(logits, answer_idx)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_epoch += float(loss)\n",
        "        optimizer.zero_grad()\n",
        "        batch_number = batch_idx + 1\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.03f}%\".format(100 * (correct_prediction_count /(batch_number*batch_size))),\n",
        "            loss=\"{:.03f}\".format(float(loss_epoch /batch_number)),\n",
        "            num_correct=correct_prediction_count,\n",
        "            lr=\"{:.03f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        batch_bar.update() \n",
        "        torch.cuda.empty_cache()\n",
        "    batch_bar.close()\n",
        "    train_acc = 100 * (correct_prediction_count / (len(train_loader) * batch_size))\n",
        "    dev_acc = val_acc(vbert_mod, val_loader, dev_batch_size)\n",
        "\n",
        "    fileName = get_fileName(epoch, isDev=False)\n",
        "    torch.save({'epoch': epoch, 'model_state_dict': vbert_mod.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'loss': loss_epoch/len(train_loader),},  f'../{fileName}')\n",
        "    \n",
        "    if dev_acc > best_dev_acc:\n",
        "        fileName = get_fileName(epoch, isDev=True)\n",
        "        best_dev_acc = dev_acc\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': vbert_mod.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'loss': loss_epoch/len(train_loader),},  f'../{fileName}')\n",
        "    epoch += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}